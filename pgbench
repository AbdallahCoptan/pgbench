#!/usr/bin/env python3
#
# Copyright (c) 2016 MagicStack Inc.
# All rights reserved.
#
# See LICENSE for details.
##


import argparse
import collections
import datetime
import glob
import json
import math
import os.path
import platform
import re
import string
import subprocess
import sys
import warnings

import numpy as np


def platform_info():
    machine = platform.machine()
    processor = platform.processor()
    system = platform.system()

    cpuinfo_f = '/proc/cpuinfo'

    if (processor in {machine, 'unknown'} and os.path.exists(cpuinfo_f)):
        with open(cpuinfo_f, 'rt') as f:
            for line in f:
                if line.startswith('model name'):
                    _, _, p = line.partition(':')
                    processor = p.strip()
                    break

    if 'Linux' in system:

        with warnings.catch_warnings():
            # see issue #1322 for more information
            warnings.filterwarnings(
                'ignore',
                'dist\(\) and linux_distribution\(\) '
                'functions are deprecated .*',
                PendingDeprecationWarning,
            )
            distname, distversion, distid = platform.dist('')

        distribution = '{} {}'.format(distname, distversion).strip()

    else:
        distribution = None

    data = {
        'cpu': processor,
        'arch': machine,
        'system': '{} {}'.format(system, platform.release()),
        'distribution': distribution
    }

    return data


def weighted_quantile(values, quantiles, weights):
    """Very close to np.percentile, but supports weights.

    :param values: np.array with data
    :param quantiles: array-like with many quantiles needed,
           quantiles should be in [0, 1]!
    :param weights: array-like of the same length as `array`
    :return: np.array with computed quantiles.
    """
    values = np.array(values)
    quantiles = np.array(quantiles)
    weights = np.array(weights)
    assert np.all(quantiles >= 0) and np.all(quantiles <= 1), \
                 'quantiles should be in [0, 1]'

    weighted_quantiles = np.cumsum(weights) - 0.5 * weights
    weighted_quantiles -= weighted_quantiles[0]
    weighted_quantiles /= weighted_quantiles[-1]

    return np.interp(quantiles, weighted_quantiles, values)


def calc_latency_stats(queries, rows, duration, min_latency, max_latency,
                       latency_stats, output_format='text'):
    arange = np.arange(len(latency_stats))

    mean_latency = np.average(arange, weights=latency_stats)
    variance = np.average((arange - mean_latency) ** 2, weights=latency_stats)
    latency_std = math.sqrt(variance)
    latency_cv = latency_std / mean_latency

    percentiles = [25, 50, 75, 90, 99, 99.99]
    percentile_data = []

    quantiles = weighted_quantile(arange, [p / 100 for p in percentiles],
                                  weights=latency_stats)

    for i, percentile in enumerate(percentiles):
        percentile_data.append((percentile, round(quantiles[i] / 100, 3)))

    data = dict(
        duration=round(duration, 2),
        queries=queries,
        qps=round(queries / duration, 2),
        rps=round(rows / duration, 2),
        latency_min=round(min_latency / 100, 3),
        latency_mean=round(mean_latency / 100, 3),
        latency_max=round(max_latency / 100, 3),
        latency_std=round(latency_std / 100, 3),
        latency_cv=round(latency_cv * 100, 2),
        latency_percentiles=percentile_data
    )

    return data


def format_text(data):
    data = dict(data)

    data['latency_percentiles'] = '; '.join(
        '{}% under {}ms'.format(*v) for v in data['latency_percentiles'])

    output = '''\
{queries} queries in {duration} seconds
Latency: min {latency_min}ms; max {latency_max}ms; mean {latency_mean}ms; \
std: {latency_std}ms ({latency_cv}%)
Latency distribution: {latency_percentiles}
Queries/sec: {qps}
Rows/sec: {rps}
'''.format(**data)

    return output


def process_results(results):
    lat_data = json.loads(results)
    latency_stats = np.array(lat_data['latency_stats'])

    return calc_latency_stats(
        lat_data['queries'], lat_data['rows'], lat_data['duration'],
        lat_data['min_latency'], lat_data['max_latency'], latency_stats)


def format_report(data, target_file):
    tpl_path = os.path.join(os.path.dirname(__file__), 'report', 'report.html')

    with open(tpl_path, 'r') as f:
        tpl = string.Template(f.read())

    now = datetime.datetime.now()
    date = now.strftime('%c')
    platform = '{system} ({dist}, {arch}) on {cpu}'.format(
        system=data['platform']['system'],
        dist=data['platform']['distribution'],
        arch=data['platform']['arch'],
        cpu=data['platform']['cpu'],
    )

    i = 0

    entries = collections.OrderedDict()

    for benchmark in data['benchmarks']:
        entry = {}

        bname = benchmark['name']

        try:
            entry = entries[bname]
        except KeyError:
            entry = entries[bname] = {
                'name': bname,
                'benchmarks': collections.OrderedDict((
                    ('Queries/sec', []),
                    ('Rows/sec', []),
                    ('Min latency', []),
                    ('Mean latency', []),
                    ('Max latency', []),
                    ('Latency variation', []),
                ))
            }

        brecords = entry['benchmarks']
        variations = benchmark['variations']
        i = 0

        for concurrency in data['concurrency_levels']:
            for query in data['querynames']:
                variation = variations[i]
                i += 1

                brecords['Queries/sec'].append(
                    variation['qps'])
                brecords['Rows/sec'].append(
                    '{}'.format(variation['rps']))
                brecords['Min latency'].append(
                    '{}ms'.format(variation['latency_min']))
                brecords['Mean latency'].append(
                    '{}ms'.format(variation['latency_mean']))
                brecords['Max latency'].append(
                    '{}ms'.format(variation['latency_max']))
                brecords['Latency variation'].append('{}ms ({}%)'.format(
                    variation['latency_std'], variation['latency_cv']))

    vc = len(data['concurrency_levels']) * len(data['querynames'])

    variations_th = []
    for concurrency in data['concurrency_levels']:
        for queryname in data['querynames']:
            variations_th.append(
                '<th>{}</th>'.format(
                    '{} x{}'.format(queryname, concurrency)
                )
            )

    record_trs = []
    for bname, entry in entries.items():
        record_trs.append(
            '''<tr class="benchmark">
                <td>{name}</td>
                {empty_tds}
            </tr>'''.format(name=bname, empty_tds='<td></td>' * vc)
        )

        for metric, metric_data in entry['benchmarks'].items():
            record_trs.append(
                '<tr class="metric"><td>{metric}</td>{data}</tr>'.format(
                    metric=metric,
                    data='\n'.join('<td>{}</td>'.format(v)
                                   for v in metric_data)
                )
            )

    table = '''
        <table class="results">
            <thead>
                <tr>
                    <th></th>
                    {variations_header}
                </tr>
            </thead>
            <tbody>
                {records}
            </tbody>
        </table>
    '''.format(variations_header='\n'.join(variations_th),
               records='\n'.join(record_trs))

    output = tpl.safe_substitute(
        __BENCHMARK_DATE__=date,
        __BENCHMARK_PLATFORM__=platform,
        __BENCHMARK_DATA_TABLE__=table,
        __BENCHMARK_DATA_JSON__=json.dumps(data)
    )

    with open(target_file, 'wt') as f:
        f.write(output)


BENCHMARKS = [
    'golang-libpq',
    'golang-pgx',
    'python-aiopg',
    'python-asyncpg',
    'python-psycopg',
    'python-postgresql',
    'nodejs-pg',
    'nodejs-pg-native',
]


def main():
    parser = argparse.ArgumentParser(
        description='pg driver implementation benchmark runner')
    parser.add_argument(
        '--concurrency-levels', type=str, default='10',
        help='comma-separated concurrency-levels')
    parser.add_argument(
        '--warmup-time', type=int, default=5,
        help='duration of warmup period for each benchmark in seconds')
    parser.add_argument(
        '--duration', type=int, default=30,
        help='duration of each benchmark in seconds')
    parser.add_argument(
        '--timeout', default=2, type=int,
        help='server timeout in seconds')
    parser.add_argument(
        '--save-json', '-J', type=str,
        help='path to save benchmark results in JSON format')
    parser.add_argument(
        '--save-html', '-H', type=str,
        help='path to save benchmark results in HTML format')
    parser.add_argument(
        '--pghost', type=str, default='127.0.0.1',
        help='PostgreSQL server host')
    parser.add_argument(
        '--pgport', type=int, default=5432,
        help='PostgreSQL server port')
    parser.add_argument(
        '--pguser', type=str, default='postgres',
        help='PostgreSQL server user')
    parser.add_argument(
        '--queryfiles', help='comma-separated list of files with queries')
    parser.add_argument(
        'benchmark', help='benchmark(s) to run (omit to run all benchmarks)',
        nargs='*')

    args = parser.parse_args()

    if args.benchmark:
        benchmarks_to_run = [re.compile(b) for b in args.benchmark]
    else:
        benchmarks_to_run = [re.compile(re.escape(b)) for b in BENCHMARKS]

    mydir = os.path.dirname(os.path.realpath(__file__))

    concurrency_levels = \
        [int(cl) for cl in sorted(args.concurrency_levels.split(','))]

    if args.queryfiles:
        queryfiles = args.queryfiles.split(',')
    else:
        queryfiles = [
            os.path.join(mydir, f)
            for f in glob.glob(os.path.join(mydir, 'queries', '*.json'))
        ]

    benchmarks_data = []

    variations = []
    querynames = []

    for concurrency in concurrency_levels:
        for queryfile in sorted(queryfiles):
            queryname = os.path.basename(queryfile)
            querynames.append(queryname)

            variations.append({
                'title': '{}, concurrency {}'.format(
                    queryname, concurrency
                ),
                'concurrency': concurrency,
                'queryfile': queryfile,
            })

    for benchmark in BENCHMARKS:
        if not any(b.match(benchmark) for b in benchmarks_to_run):
            continue

        msg = 'Running {} benchmarks...'.format(benchmark)
        print(msg)
        print('=' * len(msg))
        print()

        platform, _, driver = benchmark.partition('-')
        runner = os.path.join(mydir, 'pgbench_{}'.format(platform))

        runner_args = {
            'warmup-time': args.warmup_time,
            'duration': args.duration,
            'timeout': args.timeout,
            'pghost': args.pghost,
            'pgport': args.pgport,
            'pguser': args.pguser,
            'output-format': 'json'
        }

        runner_switches = ['--{}={}'.format(k.replace('_', '-'), v)
                           for k, v in runner_args.items()]

        benchmark_data = {
            'name': benchmark,
            'variations': []
        }

        benchmarks_data.append(benchmark_data)

        for variation in variations:
            print(variation['title'])
            print('-' * len(variation['title']))
            print()
            cmdline = [runner] + runner_switches + \
                      ['--concurrency={}'.format(variation['concurrency'])] + \
                      [driver, variation['queryfile']]
            print(' '.join(cmdline))

            runner_proc = subprocess.run(
                cmdline, stdout=subprocess.PIPE, stderr=sys.stderr)

            if runner_proc.returncode != 0:
                msg = 'fatal: benchmark runner exited with exit code {}'.\
                        format(runner_proc.returncode)
                print(msg)
                sys.exit(runner_proc.returncode)

            data = process_results(runner_proc.stdout.decode('utf-8'))
            benchmark_data['variations'].append(data)

            print(format_text(data))

    if args.save_json or args.save_html:
        plat_info = platform_info()

        benchmarks_data = {
            'date': '%Y-%m-%dT%H:%M:%S%z',
            'duration': args.duration,
            'platform': plat_info,
            'concurrency_levels': concurrency_levels,
            'querynames': querynames,
            'benchmarks': benchmarks_data,
        }

    if args.save_json:
        with open(args.save_json, 'w') as f:
            json.dump(benchmarks_data, f)

    if args.save_html:
        format_report(benchmarks_data, args.save_html)


if __name__ == '__main__':
    main()
